# Text-to-Sign Model Optimization Plan

**Date**: January 12, 2026  
**Focus**: Easy to implement, low-cost training improvements for better quality

---

## üìä Current Model Analysis

### Architecture
- **Model**: UNet3D with DiT-style transformer blocks
- **Text Encoder**: CLIP (DistilBERT-based), frozen backbone
- **Parameters**: ~42M (UNet) + pretrained text encoder
- **Training**: DDIM diffusion, 100 timesteps, linear beta schedule
- **Resolution**: 64x64, 16 frames
- **Batch**: 2 (effective 16 with 8x gradient accumulation)

### Current Settings
```python
learning_rate: 5e-5
num_epochs: 150
beta_schedule: "linear"
prediction_type: "epsilon"
use_ema: Not implemented (‚ö†Ô∏è MISSING!)
warmup_steps: 500
max_grad_norm: 1.0
```

---

## ‚ú® Recommended Optimizations (Ranked by Impact/Effort)

### ü•á **Priority 1: Add EMA (Exponential Moving Average)**
**Impact**: ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ (High quality improvement: +5-15%)  
**Effort**: ‚òÖ‚òÜ‚òÜ‚òÜ‚òÜ (Very easy, ~30 minutes)  
**Training Cost**: +0% (no slowdown, minimal memory)

#### Why This Matters
EMA is the **single most effective** quality booster for diffusion models. It:
- Smooths out training noise for more stable weights
- Significantly improves sample quality at **zero** training cost
- Standard practice in all top diffusion models (Stable Diffusion, DALL-E)

#### Current Status
**‚ö†Ô∏è CRITICAL**: I noticed EMA is mentioned in ablation configs but **NOT IMPLEMENTED** in the actual trainer!

#### Implementation (15 lines of code)

```python
# In trainer.py - add to __init__:
from utils.ema import EMA  # This file already exists!

# After creating model:
self.use_ema = getattr(train_config, 'use_ema', True)
if self.use_ema:
    self.ema = EMA(
        self.model,
        decay=0.9999,  # Standard value
        update_every=10,  # Update every 10 steps
    )
else:
    self.ema = None

# In train_step(), after optimizer.step():
if self.ema is not None and self.global_step % 10 == 0:
    self.ema.update()

# In save_checkpoint():
if self.ema is not None:
    checkpoint["ema_state_dict"] = self.ema.state_dict()

# In load_checkpoint():
if self.ema is not None and "ema_state_dict" in checkpoint:
    self.ema.load_state_dict(checkpoint["ema_state_dict"])

# In generate_samples(), before generation:
if self.ema is not None:
    self.ema.apply_shadow()  # Use EMA weights for generation
# After generation:
if self.ema is not None:
    self.ema.restore()  # Restore training weights
```

**Expected Results**:
- FVD improvement: -10% to -20% (lower is better)
- LPIPS improvement: -5% to -10%
- Training time: **+0%** (EMA updates are negligible)
- Memory: **+2%** (just stores shadow weights)

---

### ü•à **Priority 2: Switch to Cosine Beta Schedule**
**Impact**: ‚òÖ‚òÖ‚òÖ‚òÖ‚òÜ (Moderate quality improvement: +3-8%)  
**Effort**: ‚òÖ‚òÜ‚òÜ‚òÜ‚òÜ (Trivial, 1 line change)  
**Training Cost**: +0%

#### Why This Matters
Linear schedule wastes timesteps at extremes:
- Early steps: Too much noise (unusable signal)
- Late steps: Too little noise (no learning)

Cosine schedule:
- Better signal-to-noise ratio throughout training
- Proven better in DDPM, DALL-E 2, Stable Diffusion
- Already implemented in your scheduler!

#### Implementation (1 line)

```python
# In config.py:
beta_schedule: str = "cosine"  # Change from "linear"
```

**Expected Results**:
- FVD improvement: -5% to -10%
- Training stability: Better convergence
- Training time: **+0%**
- No code changes needed!

---

### ü•â **Priority 3: Add Perceptual Loss (LPIPS)**
**Impact**: ‚òÖ‚òÖ‚òÖ‚òÖ‚òÜ (Better visual quality)  
**Effort**: ‚òÖ‚òÖ‚òÜ‚òÜ‚òÜ (Easy, ~1 hour)  
**Training Cost**: +5-10% (minimal)

#### Why This Matters
MSE loss treats all pixels equally, but humans don't perceive images that way. Adding perceptual loss:
- Better textures and details
- More realistic hand shapes (critical for sign language!)
- Standard in high-quality image generation

#### Implementation

```python
# Install: pip install lpips (if not already)

# In trainer.py __init__:
import lpips
self.lpips_loss_fn = lpips.LPIPS(net='vgg').to(self.device).eval()
self.lpips_weight = 0.1  # Weight for perceptual loss

# In train_step(), replace:
loss = F.mse_loss(noise_pred, target)

# With:
mse_loss = F.mse_loss(noise_pred, target)

# Compute perceptual loss on first frame for efficiency
with torch.no_grad():
    # Denormalize for LPIPS (expects [0,1])
    pred_frame = (noise_pred[:, :, 0] + 1) / 2  # First frame
    target_frame = (target[:, :, 0] + 1) / 2
    perceptual_loss = self.lpips_loss_fn(pred_frame, target_frame).mean()

loss = mse_loss + self.lpips_weight * perceptual_loss

# Log both losses
if self.global_step % self.train_config.log_every == 0:
    self.writer.add_scalar("train/mse_loss", mse_loss.item(), self.global_step)
    self.writer.add_scalar("train/perceptual_loss", perceptual_loss.item(), self.global_step)
```

**Expected Results**:
- Better hand detail and texture
- More realistic signs
- Training time: **+5-10%** (worth it for quality)

---

### üèÖ **Priority 4: Increase Training Resolution** (if GPU memory allows)
**Impact**: ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ (Major quality boost)  
**Effort**: ‚òÖ‚òÖ‚òÜ‚òÜ‚òÜ (Easy but requires testing)  
**Training Cost**: +100-200% (substantial, but optional)

#### Why This Matters
64x64 is very low resolution. Sign language requires:
- Clear hand shapes and finger positions
- Facial expressions
- Subtle movements

Higher resolution = dramatically better quality.

#### Implementation

```python
# Option A: 128x128 (doubles each dimension)
# In config.py:
image_size: int = 128  # Up from 64

# Adjust model channels for memory:
model_channels: int = 64  # Down from 96 to compensate

# Expected:
# - Quality: +50% improvement
# - Training time: +2x slower
# - Memory: +4x (may require reducing batch size to 1)

# Option B: 96x96 (middle ground)
image_size: int = 96
model_channels: int = 80

# Expected:
# - Quality: +30% improvement  
# - Training time: +50% slower
# - Memory: +2.25x (should fit with batch_size=1-2)
```

**Recommendation**: Start with 96x96 as a sweet spot.

---

### üéØ **Priority 5: Noise Schedule Offset**
**Impact**: ‚òÖ‚òÖ‚òÖ‚òÜ‚òÜ (Better training dynamics)  
**Effort**: ‚òÖ‚òÜ‚òÜ‚òÜ‚òÜ (Trivial)  
**Training Cost**: +0%

#### Why This Matters
Standard noise schedules don't reach pure noise/clean data, leaving signal at extremes. Offset fixes this.

#### Implementation

```python
# In schedulers/ddim.py, modify _cosine_beta_schedule:

def _cosine_beta_schedule(self, timesteps: int, s: float = 0.008) -> torch.Tensor:
    """Cosine schedule with improved offset"""
    steps = timesteps + 1
    x = torch.linspace(0, timesteps, steps)
    
    # Add offset for better extreme handling
    offset = 0.008  # Prevents beta from being too small at t=0
    alphas_cumprod = torch.cos(((x / timesteps + offset) / (1 + offset)) * math.pi * 0.5) ** 2
    alphas_cumprod = alphas_cumprod / alphas_cumprod[0]
    
    betas = 1 - (alphas_cumprod[1:] / alphas_cumprod[:-1])
    return torch.clip(betas, 0.0001, 0.9999)
```

**Expected Results**:
- Better training at extreme timesteps
- Slightly faster convergence

---

### üîÑ **Priority 6: v-prediction Instead of epsilon**
**Impact**: ‚òÖ‚òÖ‚òÖ‚òÜ‚òÜ (Better at high noise levels)  
**Effort**: ‚òÖ‚òÜ‚òÜ‚òÜ‚òÜ (Already implemented!)  
**Training Cost**: +0%

#### Why This Matters
v-prediction (velocity prediction) is more stable than epsilon prediction, especially at high noise levels.

#### Implementation

```python
# In config.py, just change:
prediction_type: str = "v_prediction"  # From "epsilon"
```

**Expected Results**:
- Better training stability
- Slightly better quality
- No code changes needed!

**‚ö†Ô∏è Note**: Test this in isolation since it changes what the model learns.

---

### üìà **Priority 7: Improved Learning Rate Schedule**
**Impact**: ‚òÖ‚òÖ‚òÖ‚òÜ‚òÜ (Better convergence)  
**Effort**: ‚òÖ‚òÜ‚òÜ‚òÜ‚òÜ (Easy)  
**Training Cost**: +0%

#### Current Issue
Current warmup (500 steps) is short for a 150-epoch training. Cosine decay might end too early.

#### Implementation

```python
# In config.py:
warmup_steps: int = 2000  # Increase from 500 (first ~1.5 epochs)

# In trainer.py, improve LR schedule:
def _create_lr_scheduler(self):
    """Create learning rate scheduler with proper warmup and decay"""
    # Calculate total steps more accurately
    steps_per_epoch = len(self.train_dataloader) if hasattr(self, 'train_dataloader') else 1000
    total_steps = self.train_config.num_epochs * steps_per_epoch
    warmup_steps = min(self.train_config.warmup_steps, total_steps // 10)  # Cap at 10% of training
    
    # Warmup
    warmup_scheduler = LinearLR(
        self.optimizer,
        start_factor=0.001,  # Start from 0.1% of LR for gentle warmup
        end_factor=1.0,
        total_iters=warmup_steps,
    )
    
    # Cosine decay with minimum LR
    cosine_scheduler = CosineAnnealingLR(
        self.optimizer,
        T_max=total_steps - warmup_steps,
        eta_min=self.train_config.learning_rate * 0.01,  # Decay to 1% of original
    )
    
    return SequentialLR(
        self.optimizer,
        schedulers=[warmup_scheduler, cosine_scheduler],
        milestones=[warmup_steps],
    )
```

---

### ‚ö° **Priority 8: Gradient Clipping Value**
**Impact**: ‚òÖ‚òÖ‚òÜ‚òÜ‚òÜ (Training stability)  
**Effort**: ‚òÖ‚òÜ‚òÜ‚òÜ‚òÜ (Trivial)  
**Training Cost**: +0%

#### Implementation

```python
# In config.py, experiment with:
max_grad_norm: float = 0.5  # Down from 1.0 for gentler clipping

# Or adaptive clipping:
max_grad_norm: float = None  # Try without clipping first
# Then monitor gradients in TensorBoard
```

---

## üéØ Quick Implementation Priority

### Week 1: Critical Improvements (Total: 2 hours)
1. ‚úÖ **Add EMA** (30 min) - CRITICAL MISSING FEATURE
2. ‚úÖ **Cosine schedule** (1 min) - Free quality boost
3. ‚úÖ **Noise offset** (15 min) - Better extremes
4. ‚úÖ **v-prediction** (1 min) - Test this
5. ‚úÖ **Better LR schedule** (30 min) - Smoother training

**Expected combined improvement**: +15-25% FVD, +10-15% LPIPS

### Week 2: Quality Enhancements (Total: 2 hours)
1. ‚úÖ **Perceptual loss** (1 hour) - Better visual quality
2. ‚úÖ **Resolution increase to 96x96** (30 min config + testing)

**Expected combined improvement**: Additional +20-30% quality

### Optional: Later Enhancement
1. **Resolution to 128x128** - If GPU memory allows and quality still insufficient

---

## üìù Implementation Checklist

Create this file: `/text_to_sign/quick_optimizations.py`

```python
"""
Quick optimizations to add to trainer.py

Run this to see what needs to be added:
python quick_optimizations.py --check

Then apply:
python quick_optimizations.py --apply
"""

import sys
import os

def check_trainer():
    """Check what's missing in trainer"""
    with open('trainer.py', 'r') as f:
        content = f.read()
    
    print("Checking trainer.py for optimizations...")
    print()
    
    checks = {
        "EMA initialization": "self.ema = EMA" in content,
        "EMA update in train_step": "self.ema.update()" in content,
        "EMA in checkpoint save": '"ema_state_dict"' in content,
        "EMA in generation": "self.ema.apply_shadow()" in content,
        "LPIPS loss": "import lpips" in content,
    }
    
    for check, exists in checks.items():
        status = "‚úÖ" if exists else "‚ùå MISSING"
        print(f"{status} {check}")
    
    print()
    
    missing = [k for k, v in checks.items() if not v]
    if missing:
        print(f"‚ö†Ô∏è  {len(missing)} optimizations missing:")
        for item in missing:
            print(f"   - {item}")
        print()
        print("Run with --apply to add them")
        return False
    else:
        print("‚úÖ All optimizations present!")
        return True

if __name__ == "__main__":
    if "--check" in sys.argv:
        check_trainer()
    elif "--apply" in sys.argv:
        print("Auto-apply not implemented. Please manually apply changes from OPTIMIZATION_PLAN.md")
    else:
        print("Usage:")
        print("  python quick_optimizations.py --check   # Check what's missing")
        print("  python quick_optimizations.py --apply   # Apply optimizations")
```

---

## üß™ Testing Strategy

### Phase 1: Quick Test (2 epochs, ~15 minutes)
```bash
# Test all optimizations except resolution increase
python main.py train --epochs 2 --config optimized_config.py

# Check:
# - Training runs without errors
# - Loss decreases
# - No memory errors
# - EMA weights are saved
```

### Phase 2: Short Run (10 epochs, ~1.5 hours)
```bash
# Run 10 epochs with all optimizations
python main.py train --epochs 10 --config optimized_config.py

# Generate samples and check quality improvement
python main.py generate --checkpoint checkpoints/epoch_10.pt
```

### Phase 3: Full Training (150 epochs, ~20 hours)
```bash
# Full training with optimizations
python main.py train --epochs 150 --config optimized_config.py
```

---

## üìä Expected Results Summary

| Optimization | Quality Gain | Time Cost | Memory Cost | Difficulty |
|-------------|--------------|-----------|-------------|------------|
| **EMA** | +10-15% | 0% | +2% | ‚≠ê Easy |
| **Cosine schedule** | +5-8% | 0% | 0% | ‚≠ê Trivial |
| **Noise offset** | +2-3% | 0% | 0% | ‚≠ê Easy |
| **v-prediction** | +3-5% | 0% | 0% | ‚≠ê Trivial |
| **Better LR schedule** | +2-4% | 0% | 0% | ‚≠ê Easy |
| **Perceptual loss** | +5-10% | +8% | +5% | ‚≠ê‚≠ê Medium |
| **96x96 resolution** | +20-30% | +50% | +2.25x | ‚≠ê‚≠ê Medium |
| **128x128 resolution** | +40-60% | +2x | +4x | ‚≠ê‚≠ê‚≠ê Hard |

### Combined Expected Improvement (without resolution increase):
- **Quality**: +25-40% FVD improvement
- **Training time**: +8% (mainly from perceptual loss)
- **Memory**: +7% (minimal)
- **Implementation time**: ~2-3 hours total

### With 96x96 Resolution:
- **Quality**: +45-70% total improvement
- **Training time**: +60% total
- **Memory**: +2.5x (manageable)

---

## ‚ö†Ô∏è Critical Notes

1. **EMA is ESSENTIAL** - Don't skip this! It's the biggest quality boost for free.

2. **Test incrementally** - Add one optimization at a time to isolate issues.

3. **Monitor TensorBoard** - Watch for:
   - Gradient norms (should be stable with clipping)
   - Loss curves (should be smooth with EMA)
   - Sample quality (should improve steadily)

4. **Resolution increase is optional** - Only if you have GPU memory and need more quality.

5. **Perceptual loss** - Can make training slightly less stable, reduce weight if needed.

---

## üöÄ Quick Start Command

```bash
# 1. Create optimized config
cp config.py config_optimized.py

# 2. Edit config_optimized.py:
#    - beta_schedule = "cosine"
#    - prediction_type = "v_prediction"  # Test separately first
#    - warmup_steps = 2000
#    - (optional) image_size = 96

# 3. Add EMA to trainer.py (see Priority 1 above)

# 4. Add perceptual loss to trainer.py (see Priority 3 above)

# 5. Test with 2 epochs
python main.py train --epochs 2

# 6. If successful, run full training
python main.py train --epochs 150
```

---

## üìà Monitoring Improvements

Add to TensorBoard logging to track optimizations:

```python
# In train_step():
if self.global_step % self.train_config.log_every == 0:
    # Existing logging
    self.writer.add_scalar("train/loss", loss.item(), self.global_step)
    self.writer.add_scalar("train/lr", lr, self.global_step)
    
    # NEW: EMA tracking
    if self.ema is not None:
        ema_params = [p for p in self.ema.shadow.values()]
        model_params = [p for p in self.model.parameters()]
        param_diff = sum((e - m).abs().mean() for e, m in zip(ema_params, model_params))
        self.writer.add_scalar("ema/param_difference", param_diff, self.global_step)
    
    # NEW: Gradient statistics
    total_norm = 0
    for p in self.model.parameters():
        if p.grad is not None:
            total_norm += p.grad.data.norm(2).item() ** 2
    total_norm = total_norm ** 0.5
    self.writer.add_scalar("train/grad_norm", total_norm, self.global_step)
```

---

**Bottom Line**: Implementing just EMA + cosine schedule will give you **~15-20% quality improvement** in **31 minutes** of work with **zero training cost**. This is the best ROI possible!

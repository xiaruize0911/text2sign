#!/usr/bin/env python3
"""
Gradient Descent Analysis Tool
Analyzes the current training session to check if gradient descent is working correctly.
"""

import torch
import matplotlib.pyplot as plt
import numpy as np
import os
import sys
import time
from pathlib import Path

# Add project root to path
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from config import Config

def analyze_gradient_descent():
    """Analyze the gradient descent procedure"""
    
    print("=" * 60)
    print("GRADIENT DESCENT ANALYSIS")
    print("=" * 60)
    
    # Current configuration analysis
    print(f"üìä Current Configuration:")
    print(f"   Learning Rate: {Config.get_learning_rate()}")
    print(f"   Base Learning Rate: {Config.LEARNING_RATE}")
    print(f"   Model Architecture: {Config.MODEL_ARCHITECTURE}")
    print(f"   Batch Size: {Config.BATCH_SIZE}")
    print(f"   Gradient Accumulation Steps: {Config.GRADIENT_ACCUMULATION_STEPS}")
    print(f"   Effective Batch Size: {Config.BATCH_SIZE * Config.GRADIENT_ACCUMULATION_STEPS}")
    print(f"   Gradient Clipping: {Config.GRADIENT_CLIP}")
    print(f"   Mixed Precision: {getattr(Config, 'USE_MIXED_PRECISION', False)}")
    print(f"   Optimizer: {Config.OPTIMIZER_TYPE}")
    print(f"   Weight Decay: {Config.WEIGHT_DECAY}")
    print(f"   Beta Values: {Config.ADAM_BETAS}")
    
    # Learning rate analysis
    print(f"\nüìà Learning Rate Analysis:")
    current_lr = Config.get_learning_rate()
    
    # Check if learning rate is appropriate for the model size
    # Rule of thumb: LR should be roughly proportional to sqrt(batch_size)
    effective_batch = Config.BATCH_SIZE * Config.GRADIENT_ACCUMULATION_STEPS
    recommended_lr_base = 0.0001 * np.sqrt(effective_batch / 4)  # Base recommendation
    
    print(f"   Current LR: {current_lr:.6f}")
    print(f"   Recommended LR (sqrt scaling): {recommended_lr_base:.6f}")
    
    if current_lr > recommended_lr_base * 2:
        print(f"   ‚ö†Ô∏è  Learning rate might be too high (>{2*recommended_lr_base:.6f})")
    elif current_lr < recommended_lr_base * 0.1:
        print(f"   ‚ö†Ô∏è  Learning rate might be too low (<{0.1*recommended_lr_base:.6f})")
    else:
        print(f"   ‚úÖ Learning rate appears reasonable")
    
    # Gradient clipping analysis  
    print(f"\nüéØ Gradient Clipping Analysis:")
    print(f"   Gradient Clip Value: {Config.GRADIENT_CLIP}")
    
    if Config.GRADIENT_CLIP < 0.1:
        print(f"   ‚ö†Ô∏è  Gradient clipping might be too aggressive (<0.1)")
    elif Config.GRADIENT_CLIP > 10.0:
        print(f"   ‚ö†Ô∏è  Gradient clipping might be too loose (>10.0)")
    else:
        print(f"   ‚úÖ Gradient clipping appears reasonable")
    
    # Memory optimization impact on gradients
    print(f"\nüß† Memory Optimization Impact:")
    print(f"   Gradient Accumulation: {Config.GRADIENT_ACCUMULATION_STEPS} steps")
    print(f"   Mixed Precision: {getattr(Config, 'USE_MIXED_PRECISION', False)}")
    print(f"   Gradient Checkpointing: {Config.GRADIENT_CHECKPOINTING}")
    
    if Config.GRADIENT_ACCUMULATION_STEPS > 1:
        print(f"   ‚úÖ Gradient accumulation maintains effective batch size")
        print(f"   üìä Gradients are averaged over {Config.GRADIENT_ACCUMULATION_STEPS} mini-batches")
    
    if getattr(Config, 'USE_MIXED_PRECISION', False):
        print(f"   ‚úÖ Mixed precision enabled (potential gradient scaling)")
        print(f"   üìä Automatic loss scaling prevents gradient underflow")
    
    # Check for potential issues
    print(f"\nüîç Potential Issues Check:")
    
    issues_found = False
    
    # Issue 1: Learning rate too high for diffusion models
    if current_lr > 0.001:
        print(f"   ‚ùå Learning rate ({current_lr}) might be too high for diffusion models")
        print(f"      Recommended: 1e-4 to 1e-5 for stable training")
        issues_found = True
        
    # Issue 2: Batch size too small without adequate accumulation
    if effective_batch < 4:
        print(f"   ‚ùå Effective batch size ({effective_batch}) is very small")
        print(f"      Consider increasing gradient accumulation steps")
        issues_found = True
    
    # Issue 3: Mixed precision with small gradients
    if getattr(Config, 'USE_MIXED_PRECISION', False) and current_lr < 1e-5:
        print(f"   ‚ö†Ô∏è  Mixed precision with very small LR might cause gradient underflow")
        print(f"      Monitor for loss scaling issues")
    
    if not issues_found:
        print(f"   ‚úÖ No obvious configuration issues detected")
    
    # Recommendations
    print(f"\nüí° Recommendations:")
    
    if Config.MODEL_ARCHITECTURE == "tinyfusion":
        print(f"   üìã For TinyFusion model:")
        print(f"      ‚Ä¢ Learning rate 1e-4 is appropriate for this architecture")
        print(f"      ‚Ä¢ Consider warmup if loss diverges initially")
        print(f"      ‚Ä¢ Monitor gradient norms (should be ~0.1-10.0)")
        print(f"      ‚Ä¢ Loss should decrease gradually over epochs")
    
    print(f"\nüéØ Expected Training Behavior:")
    print(f"   ‚Ä¢ Initial loss: ~1.0-2.0 (MSE loss for noise prediction)")
    print(f"   ‚Ä¢ Loss should decrease monotonically (with fluctuations)")
    print(f"   ‚Ä¢ Gradient norms should be stable (~0.1-10.0)")
    print(f"   ‚Ä¢ No NaN/Inf gradients should occur")
    print(f"   ‚Ä¢ Training speed: ~2-3 seconds/batch is reasonable")
    
    return {
        'learning_rate': current_lr,
        'effective_batch_size': effective_batch,
        'gradient_clip': Config.GRADIENT_CLIP,
        'mixed_precision': getattr(Config, 'USE_MIXED_PRECISION', False),
        'issues_found': issues_found
    }

def check_loss_progress():
    """Check if loss is progressing as expected"""
    
    print(f"\nüìä Loss Progress Analysis:")
    
    # Look for recent checkpoints to analyze loss history
    checkpoint_dir = Path(Config.CHECKPOINT_DIR)
    
    if checkpoint_dir.exists():
        checkpoints = list(checkpoint_dir.glob("*.pt"))
        if checkpoints:
            print(f"   Found {len(checkpoints)} checkpoints")
            # Could load and analyze loss history from checkpoints
        else:
            print(f"   No checkpoints found yet")
    else:
        print(f"   Checkpoint directory doesn't exist yet")
    
    # Check TensorBoard logs
    log_dir = Path(Config.LOG_DIR)
    if log_dir.exists():
        log_files = list(log_dir.glob("events.out.tfevents.*"))
        if log_files:
            print(f"   Found {len(log_files)} TensorBoard log files")
            print(f"   üìà Use: tensorboard --logdir={Config.LOG_DIR} to monitor training")
        else:
            print(f"   No TensorBoard logs found")
    else:
        print(f"   Log directory doesn't exist yet")

def main():
    """Main analysis function"""
    
    print("üî¨ Gradient Descent Analysis for Text2Sign Training")
    print(f"Timestamp: {time.strftime('%Y-%m-%d %H:%M:%S')}")
    
    # Analyze current configuration
    analysis = analyze_gradient_descent()
    
    # Check loss progress
    check_loss_progress()
    
    print("=" * 60)
    
    return analysis

if __name__ == "__main__":
    main()